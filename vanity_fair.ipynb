{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    p = urlopen(url)\n",
    "    s = BeautifulSoup(p)\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    a = len(soup.find_all('p'))\n",
    "    text = \"\"\n",
    "    for i in range(0, a):\n",
    "        text = text + soup.find_all('p')[i].text + \" \"\n",
    "        text = text.replace(\"\\n\",\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(url):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page)\n",
    "    links = []\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "        links.append(link.get('href'))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_story_url(search_string):\n",
    "    page = requests.get(search_string)\n",
    "    soup = BeautifulSoup(page.content)\n",
    "    links = soup.findAll(\"a\")\n",
    "    results = []\n",
    "    for link in  soup.find_all(\"a\",href=re.compile(\"(?<=/url\\?q=)(htt.*://.*)\")):\n",
    "        whole_thing = re.split(\":(?=http)\",link[\"href\"].replace(\"/url?q=\",\"\"))\n",
    "        results.append(whole_thing[0].split('&')[0])\n",
    "    rv = []\n",
    "    for result in results:\n",
    "        if \"www.vanityfair.com\" in result:\n",
    "            rv.append(result)\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(pics):\n",
    "    images = []\n",
    "    for pic in pics:\n",
    "        if 'https' in pic:\n",
    "            image = Image.open(urlopen(pic))\n",
    "            images.append(image)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_set(years):\n",
    "    search_string_root = \"https://www.google.com/search?q=vanity+fair+cover+story\"\n",
    "    search_strings = []\n",
    "    months = ['february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n",
    "    #years = [2015, 2016, 2017]\n",
    "    for month in  months:\n",
    "        for year in years:\n",
    "            search_string = search_string_root + \"+\" + month + \"+\" + str(year)\n",
    "            search_strings.append(search_string)\n",
    "\n",
    "    # get story urls\n",
    "    urls = []\n",
    "    for search_string in search_strings:\n",
    "        urls.extend(get_story_url(search_string))\n",
    "\n",
    "    urls = list(set(urls))\n",
    "    url_set = []\n",
    "    for url in urls:\n",
    "        if len(url) > 50:\n",
    "            url_set.append(url)\n",
    "\n",
    "    urls = url_set\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details(url, data):\n",
    "    p = urlopen(url)\n",
    "    s = BeautifulSoup(p)\n",
    "    img_tags = s.find_all('img')\n",
    "    t = s.find_all('script', type=\"application/ld+json\")\n",
    "    d = eval(t[0].contents[0])\n",
    "    \n",
    "    author = d['creator'][0][0]\n",
    "    try:\n",
    "        photographer = d['creator'][0][1]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        stylist = d['creator'][0][2]\n",
    "    except:\n",
    "        pass\n",
    "    date = d['dateCreated']\n",
    "    cover_image = d['image']['url']\n",
    "    tags = d['keywords']\n",
    "    headline = d['headline']\n",
    "    text = get_text(url)\n",
    "    pics = get_images([img['src'] for img in img_tags])\n",
    "    \n",
    "\n",
    "    data['author'].append(author)\n",
    "    try:\n",
    "        data['photographer'].append(photographer)\n",
    "    except:\n",
    "        data['photographer'].append(\" \")\n",
    "\n",
    "    try:\n",
    "        data['stylist'].append(stylist)\n",
    "    except:\n",
    "        data['stylist'].append(\"\")\n",
    "    data['date'].append(date)\n",
    "    data['cover_image'].append(cover_image)\n",
    "    data['tags'].append(tags)\n",
    "    data['headline'].append(headline)\n",
    "    data['text'].append(text)\n",
    "    if pics:\n",
    "        data['pics'].append(pics)\n",
    "    else:\n",
    "        data['pics'].append([])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.2 s, sys: 3.36 s, total: 49.5 s\n",
      "Wall time: 7min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## takes approx 7 minutes\n",
    "data = {'author': [], 'photographer': [], 'stylist': [], 'date': [],\n",
    "       'cover_image': [], 'tags': [], 'headline': [],\n",
    "       'text': [], 'pics': []}\n",
    "\n",
    "for url in get_url_set([2015,2016,2017]):\n",
    "    if \"rihanna\" in url and \"cuba-annie-leibovitz\" in url:\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            get_details(url,data)\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.4 s, sys: 3.58 s, total: 51.9 s\n",
      "Wall time: 7min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## takes approx 11 minutes\n",
    "data2 = {'author': [], 'photographer': [], 'stylist': [], 'date': [],\n",
    "       'cover_image': [], 'tags': [], 'headline': [],\n",
    "       'text': [], 'pics': []}\n",
    "\n",
    "for url in get_url_set([2012,2013,2014]):\n",
    "    if \"rihanna\" in url and \"cuba-annie-leibovitz\" in url:\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            get_details(url,data2)\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(data)\n",
    "df2 = pd.DataFrame(data2)\n",
    "df = df1.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['author'] != \"Vanity Fair\"]\n",
    "df = df.replace('', np.nan, regex=True)\n",
    "\n",
    "\n",
    "df = df.dropna(subset=['text'])\n",
    "df['text'] = df['text'].str[:-455] # remove Conde Nast user agreement from each interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c9caaeb06525>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'celeb_gender'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'celeb_gender'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2519\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2585\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2586\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   2758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2760\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2761\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2762\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m   3119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3121\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of '\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeriodIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "male_authors = [\"Todd Purdum\", \"Sam Kashner\", \"Rich Cohen\", \"Mitch Glazer\", \n",
    "                \"Michael Schulman\", \"Josh Duboff\", \"Jim Windolf\", \"James Fox\", \n",
    "                \"Jack Deligter, Jaime Lalinde and Mickey Stanley\", \"Alec Baldwin\", \n",
    "                \"Alex Beggs\", \"Bruce Handy\", \"Buzz Bissinger\", \"David Kamp\", \n",
    "                \"Derek Blasberg\", \"Fred Burton\"] \n",
    "\n",
    "def author_gender(row):\n",
    "    global male_authors\n",
    "    if row['author'] in male_authors:\n",
    "        return 'male'\n",
    "    return 'female'\n",
    "\n",
    "df['author_gender'] = df.apply (lambda row: author_gender(row), axis=1 )\n",
    "\n",
    "def cover(row):\n",
    "    if 'cover story' in row['tags']:\n",
    "        return \"cover\"\n",
    "    return \"DELETE\"\n",
    "df['cover'] = df.apply (lambda row: cover(row), axis=1 )\n",
    "df = df[df['cover'] == \"cover\"]\n",
    "\n",
    "\n",
    "\n",
    "df['celeb_gender'] = [0,0,0,999,0,0,1,0,0,0,1,1,0,0,0,0,0,0,1,0,0,999,0,1,0,0,0,0,0]\n",
    "df = df[df['celeb_gender']< 2]\n",
    "\n",
    "mask = df.celeb_gender ==  0\n",
    "column_name = 'celeb_gender'\n",
    "df.loc[mask, column_name] = 'female'\n",
    "df.groupby('celeb_gender').count()[['author']]\n",
    "\n",
    "mask = df.celeb_gender ==  1\n",
    "column_name = 'celeb_gender'\n",
    "df.loc[mask, column_name] = 'male'\n",
    "df.groupby('celeb_gender').count()[['author']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('author_gender').count()[['author']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.author_gender, df.celeb_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['date'].astype(str).str[0:4]\n",
    "# check if GQ years add up\n",
    "df.groupby('year').count()[['author']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_length'] = df['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['text'], inplace = True)\n",
    "df_fm = df[(df['author_gender'] == \"female\") & (df['celeb_gender'] == \"male\")]\n",
    "df_fm['pair'] = \"Female-Male\"\n",
    "df_ff = df[(df['author_gender'] == \"female\") & (df['celeb_gender'] == \"female\")]\n",
    "df_ff['pair'] = \"Female-Female\"\n",
    "df_mf = df[(df['author_gender'] == \"male\") & (df['celeb_gender'] == \"female\")]\n",
    "df_mf['pair'] = \"Male-Female\"\n",
    "df_mm = df[(df['author_gender'] == \"male\") & (df['celeb_gender'] == \"male\")]\n",
    "df_mm['pair'] = \"Male-Male\"\n",
    "\n",
    "df_f = pd.concat([df_fm, df_ff])\n",
    "df_m = pd.concat([df_mf, df_mm])\n",
    "\n",
    "plot_data = pd.concat([df_fm,df_ff, df_mf, df_mm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['text_length'], hist = False, rug = True, kde = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot(kind ='box',        # Boxplot\n",
    "               y = 'text_length',       # Y-axis - values for boxplot\n",
    "               x ='author_gender',        # X-axis - first factor\n",
    "               data = df,\n",
    "               size = 8,            # Figure size (x100px)      \n",
    "               aspect = 1,        # Width = size * aspect \n",
    "               legend_out = False)  # Make legend inside the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot(x = \"pair\", \n",
    "               y=\"text_length\", \n",
    "               data=plot_data, \n",
    "               kind=\"box\",\n",
    "               size = 8,\n",
    "               aspect = 1\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "original = df\n",
    "original['pair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['tokens'] = df['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "from nltk.corpus import stopwords\n",
    "# tokenize\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "tokens = flatten(flatten(df['tokens'].tolist()))\n",
    "# remove stop words\n",
    "stopwords = stopwords.words('english')\n",
    "clean_tokens = []\n",
    "for token in tokens:\n",
    "    t = token.lower()\n",
    "    if t not in stopwords and len(t) > 1: # remove stopwords and punctuation\n",
    "        clean_tokens.append(t)\n",
    "freq = nltk.FreqDist(clean_tokens)\n",
    "tokens_all = clean_tokens\n",
    "freq.plot(20,cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df_mm\n",
    "df['tokens'] = df['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "from nltk.corpus import stopwords\n",
    "# tokenize\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "tokens = flatten(flatten(df['tokens'].tolist()))\n",
    "# remove stop words\n",
    "stopwords = stopwords.words('english')\n",
    "clean_tokens = []\n",
    "for token in tokens:\n",
    "    t = token.lower()\n",
    "    if t not in stopwords and len(t) > 1: # remove stopwords and punctuation\n",
    "        clean_tokens.append(t)\n",
    "freq = nltk.FreqDist(clean_tokens)\n",
    "tokens_mm = clean_tokens\n",
    "\n",
    "\n",
    "freq.plot(20,cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df_mf\n",
    "df['tokens'] = df['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "from nltk.corpus import stopwords\n",
    "# tokenize\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "tokens = flatten(flatten(df['tokens'].tolist()))\n",
    "tokens_f = tokens\n",
    "# remove stop words\n",
    "stopwords = stopwords.words('english')\n",
    "clean_tokens = []\n",
    "for token in tokens:\n",
    "    t = token.lower()\n",
    "    if t not in stopwords and len(t) > 1: # remove stopwords and punctuation\n",
    "        clean_tokens.append(t)\n",
    "freq = nltk.FreqDist(clean_tokens)\n",
    "tokens_mf = clean_tokens\n",
    "\n",
    "\n",
    "freq.plot(20,cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df_ff\n",
    "df['tokens'] = df['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "from nltk.corpus import stopwords\n",
    "# tokenize\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "tokens = flatten(flatten(df['tokens'].tolist()))\n",
    "tokens_f = tokens\n",
    "# remove stop words\n",
    "stopwords = stopwords.words('english')\n",
    "clean_tokens = []\n",
    "for token in tokens:\n",
    "    t = token.lower()\n",
    "    if t not in stopwords and len(t) > 1: # remove stopwords and punctuation\n",
    "        clean_tokens.append(t)\n",
    "freq = nltk.FreqDist(clean_tokens)\n",
    "tokens_ff = clean_tokens\n",
    "\n",
    "\n",
    "freq.plot(20,cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df_mm\n",
    "df['tokens'] = df['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "from nltk.corpus import stopwords\n",
    "# tokenize\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "tokens = flatten(flatten(df['tokens'].tolist()))\n",
    "tokens_f = tokens\n",
    "# remove stop words\n",
    "stopwords = stopwords.words('english')\n",
    "clean_tokens = []\n",
    "for token in tokens:\n",
    "    t = token.lower()\n",
    "    if t not in stopwords and len(t) > 1: # remove stopwords and punctuation\n",
    "        clean_tokens.append(t)\n",
    "freq = nltk.FreqDist(clean_tokens)\n",
    "tokens_fm = clean_tokens\n",
    "\n",
    "\n",
    "freq.plot(20,cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# see collocations\n",
    "text = nltk.Text(tokens_all)\n",
    "text_mm = nltk.Text(tokens_mm)\n",
    "text_ff = nltk.Text(tokens_ff)\n",
    "text_mf = nltk.Text(tokens_mf)\n",
    "text_fm = nltk.Text(tokens_fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mm.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ff.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mf.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_fm.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [text, text_mm, text_ff, text_mf, text_fm]\n",
    "words = ['sexy', 'hot', 'pretty','beautiful','woman']\n",
    "for t in texts:\n",
    "    for word in words:\n",
    "        if t == text:\n",
    "            print(\"All\" + \" \" + word)\n",
    "        if t == text_m:\n",
    "            print(\"Male\" + \" \" + word)\n",
    "        if t == text_f:\n",
    "            print(\"Female\" + \" \" + word)\n",
    "        if t.common_contexts([word]):\n",
    "                print(t.common_contexts([word]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-165ae31cdfb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbigram_measures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBigramAssocMeasures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfinder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBigramCollocationFinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_freq_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_ngrams\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbigram_measures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens_all' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(tokens_all, 10)\n",
    "finder.apply_freq_filter(10)\n",
    "finder.score_ngrams (bigram_measures.likelihood_ratio)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "whTrigrams = nltk.collocations.TrigramCollocationFinder.from_words(tokens_all)\n",
    "whTrigrams.score_ngrams(trigram_measures.student_t)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(tokens_mm, 10)\n",
    "finder.apply_freq_filter(10)\n",
    "finder.score_ngrams (bigram_measures.likelihood_ratio)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "whTrigrams = nltk.collocations.TrigramCollocationFinder.from_words(tokens_mm)\n",
    "whTrigrams.score_ngrams(trigram_measures.student_t)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(tokens_ff, 10)\n",
    "finder.apply_freq_filter(10)\n",
    "finder.score_ngrams (bigram_measures.likelihood_ratio)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "whTrigrams = nltk.collocations.TrigramCollocationFinder.from_words(tokens_ff)\n",
    "whTrigrams.score_ngrams(trigram_measures.student_t)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(tokens_mf, 10)\n",
    "finder.apply_freq_filter(10)\n",
    "finder.score_ngrams (bigram_measures.likelihood_ratio)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "whTrigrams = nltk.collocations.TrigramCollocationFinder.from_words(tokens_mf)\n",
    "whTrigrams.score_ngrams(trigram_measures.student_t)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(tokens_fm, 10)\n",
    "finder.apply_freq_filter(10)\n",
    "finder.score_ngrams (bigram_measures.likelihood_ratio)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "whTrigrams = nltk.collocations.TrigramCollocationFinder.from_words(tokens_fm)\n",
    "whTrigrams.score_ngrams(trigram_measures.student_t)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = [tokens_mm, tokens_mf, tokens_ff, tokens_fm]\n",
    "stopwords = stopwords.words('english')\n",
    "corpora_s = []\n",
    "corpora_nons = []\n",
    "for corpus in corpora:\n",
    "    s = []\n",
    "    nons = []\n",
    "    for word in corpus:\n",
    "        if word in stopwords:\n",
    "            s.append(word)\n",
    "        else:\n",
    "            nons.append(word)\n",
    "    corpora_s.append(s)\n",
    "    corpora_nons.append(nons)\n",
    "    \n",
    "def kl_divergence(X, Y):\n",
    "    P = X.copy()\n",
    "    Q = Y.copy()\n",
    "    P.columns = ['P']\n",
    "    Q.columns = ['Q']\n",
    "    df = Q.join(P).fillna(0)\n",
    "    p = df.iloc[:,1]\n",
    "    q = df.iloc[:,0]\n",
    "    D_kl = scipy.stats.entropy(p, q)\n",
    "    return D_kl\n",
    "\n",
    "def chi2_divergence(X,Y):\n",
    "    P = X.copy()\n",
    "    Q = Y.copy()\n",
    "    P.columns = ['P']\n",
    "    Q.columns = ['Q']\n",
    "    df = Q.join(P).fillna(0)\n",
    "    p = df.iloc[:,1]\n",
    "    q = df.iloc[:,0]\n",
    "    return scipy.stats.chisquare(p, q).statistic\n",
    "\n",
    "def Divergence(corpus1, corpus2, difference=\"KL\"):\n",
    "    \"\"\"Difference parameter can equal KL, Chi2, or Wass\"\"\"\n",
    "    freqP = nltk.FreqDist(corpus1)\n",
    "    P = pandas.DataFrame(list(freqP.values()), columns = ['frequency'], index = list(freqP.keys()))\n",
    "    freqQ = nltk.FreqDist(corpus2)\n",
    "    Q = pandas.DataFrame(list(freqQ.values()), columns = ['frequency'], index = list(freqQ.keys()))\n",
    "    if difference == \"KL\":\n",
    "        return kl_divergence(P, Q)\n",
    "    elif difference == \"Chi2\":\n",
    "        return chi2_divergence(P, Q)\n",
    "    elif difference == \"KS\":\n",
    "        try:\n",
    "            return scipy.stats.ks_2samp(P['frequency'], Q['frequency']).statistic\n",
    "        except:\n",
    "            return scipy.stats.ks_2samp(P['frequency'], Q['frequency'])\n",
    "    elif difference == \"Wasserstein\":\n",
    "        try:\n",
    "            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None).statistic\n",
    "        except:\n",
    "            return scipy.stats.wasserstein_distance(P['frequency'], Q['frequency'], u_weights=None, v_weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucem_illud \n",
    "import requests #for http requests\n",
    "import nltk #the Natural Language Toolkit\n",
    "import pandas #gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import wordcloud #Makes word clouds\n",
    "import numpy as np #For divergences/distances\n",
    "import scipy #For divergences/distances\n",
    "import seaborn as sns #makes our plots look nicer\n",
    "import sklearn.manifold #For a manifold plot\n",
    "from nltk.corpus import stopwords #For stopwords\n",
    "import json #For API responses\n",
    "import urllib.parse #For joining urls\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "L = []\n",
    "for p in corpora:\n",
    "    l = []\n",
    "    for q in corpora:\n",
    "        l.append(Divergence(p,q, difference = 'KL'))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = ['MM','MF','FF','FM'], index = ['MM','MF','FF','FM'])\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileids = ['MM','MF','FF','FM']\n",
    "L = []\n",
    "for p in corpora_nons:\n",
    "    l = []\n",
    "    for q in corpora_nons:\n",
    "        l.append(Divergence(p,q, difference='KL'))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = fileids, index = fileids)\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "for p in corpora_nons:\n",
    "    l = []\n",
    "    for q in corpora_nons:\n",
    "        l.append(Divergence(p,q, difference='KS'))\n",
    "    L.append(l)\n",
    "M = np.array(L)\n",
    "fig = plt.figure()\n",
    "div = pandas.DataFrame(M, columns = fileids, index = fileids)\n",
    "ax = sns.heatmap(div)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "#These are all for the cluster detection\n",
    "import sklearn\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import sklearn.cluster\n",
    "import sklearn.decomposition\n",
    "import sklearn.metrics\n",
    "\n",
    "import scipy #For hierarchical clustering and some visuals\n",
    "#import scipy.cluster.hierarchy\n",
    "import gensim#For topic modeling\n",
    "import nltk #the Natural Language Toolkit\n",
    "import requests #For downloading our datasets\n",
    "import numpy as np #for arrays\n",
    "import pandas #gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import matplotlib.cm #Still for graphics\n",
    "import seaborn as sns #Makes the graphics look nicer\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning, it\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = plot_data\n",
    "# create count vector\n",
    "ngCountVectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "newsgroupsVects = ngCountVectorizer.fit_transform(df['text'])\n",
    "\n",
    "# tf-idf\n",
    "newsgroupsTFTransformer = sklearn.feature_extraction.text.TfidfTransformer().fit(newsgroupsVects)\n",
    "newsgroupsTF = newsgroupsTFTransformer.transform(newsgroupsVects)\n",
    "\n",
    "# # vectorize\n",
    "ngTFVectorizer = sklearn.feature_extraction.text.TfidfVectorizer(stop_words='english', norm='l2')\n",
    "newsgroupsTFVects = ngTFVectorizer.fit_transform(df['text'])\n",
    "\n",
    "\n",
    "# # k-means\n",
    "numClusters = 4\n",
    "\n",
    "km = sklearn.cluster.KMeans(n_clusters=numClusters, init='k-means++')\n",
    "km.fit(newsgroupsTFVects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ngTFVectorizer.get_feature_names()\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(numClusters):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA = sklearn.decomposition.PCA\n",
    "pca = PCA(n_components = 2).fit(newsgroupsTFVects.toarray())\n",
    "reduced_data = pca.transform(newsgroupsTFVects.toarray())\n",
    "\n",
    "colordict = {'Male-Male': 'red','Male-Female': 'orange','Female-Female': 'green','Female-Male': 'blue'}\n",
    "colors = [colordict[c] for c in df['pair']]\n",
    "components = pca.components_\n",
    "keyword_ids = list(set(order_centroids[:,:10].flatten())) #Get the ids of the most distinguishing words(features) from your kmeans model.\n",
    "words = [terms[i] for i in keyword_ids]#Turn the ids into words.\n",
    "x = components[:,keyword_ids][0,:] #Find the coordinates of those words in your biplot.\n",
    "y = components[:,keyword_ids][1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "ax.scatter(reduced_data[:, 0], reduced_data[:, 1], color = colors, alpha = 0.3, label = colors)\n",
    "newsgroupsCategories = ['Male-Male', 'Male-Female', 'Female-Female','Female-Male']\n",
    "colors_p = [colordict[newsgroupsCategories[l]] for l in km.labels_]\n",
    "for i, word in enumerate(words):\n",
    "    ax.annotate(word, (x[i],y[i]))\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('True Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], color = colors_p, alpha = 0.5)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('Predicted Clusters\\n k = 4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSilhouette(n_clusters, X):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (15,5))\n",
    "    \n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "    clusterer = sklearn.cluster.KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "    \n",
    "    silhouette_avg = sklearn.metrics.silhouette_score(X, cluster_labels)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = sklearn.metrics.silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = matplotlib.cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        y_lower = y_upper + 10\n",
    "    \n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = matplotlib.cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(reduced_data[:, 0], reduced_data[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors)\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    projected_centers = pca.transform(centers)\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(projected_centers[:, 0], projected_centers[:, 1],\n",
    "                marker='o', c=\"white\", alpha=1, s=200)\n",
    "\n",
    "    for i, c in enumerate(projected_centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"PC 1\")\n",
    "    ax2.set_ylabel(\"PC 2\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "    print(\"For n_clusters = {}, The average silhouette_score is : {:.3f}\".format(n_clusters, silhouette_avg))\n",
    "    return silhouette_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = newsgroupsTFVects.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = plot_data\n",
    "# create count vector\n",
    "ngCountVectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "newsgroupsVects = ngCountVectorizer.fit_transform(df['text'])\n",
    "\n",
    "# tf-idf\n",
    "newsgroupsTFTransformer = sklearn.feature_extraction.text.TfidfTransformer().fit(newsgroupsVects)\n",
    "newsgroupsTF = newsgroupsTFTransformer.transform(newsgroupsVects)\n",
    "\n",
    "# # vectorize\n",
    "ngTFVectorizer = sklearn.feature_extraction.text.TfidfVectorizer(stop_words='english', norm='l2')\n",
    "newsgroupsTFVects = ngTFVectorizer.fit_transform(df['text'])\n",
    "\n",
    "\n",
    "# # k-means\n",
    "numClusters = 2\n",
    "\n",
    "km = sklearn.cluster.KMeans(n_clusters=numClusters, init='k-means++')\n",
    "km.fit(newsgroupsTFVects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA = sklearn.decomposition.PCA\n",
    "pca = PCA(n_components = 2).fit(newsgroupsTFVects.toarray())\n",
    "reduced_data = pca.transform(newsgroupsTFVects.toarray())\n",
    "\n",
    "colordict = {'male': 'red','female': 'blue'}\n",
    "colors = [colordict[c] for c in df['author_gender']]\n",
    "components = pca.components_\n",
    "keyword_ids = list(set(order_centroids[:,:10].flatten())) #Get the ids of the most distinguishing words(features) from your kmeans model.\n",
    "words = [terms[i] for i in keyword_ids]#Turn the ids into words.\n",
    "x = components[:,keyword_ids][0,:] #Find the coordinates of those words in your biplot.\n",
    "y = components[:,keyword_ids][1,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "ax.scatter(reduced_data[:, 0], reduced_data[:, 1], color = colors, alpha = 0.3, label = colors)\n",
    "newsgroupsCategories = ['male', 'female']\n",
    "colors_p = [colordict[newsgroupsCategories[l]] for l in km.labels_]\n",
    "for i, word in enumerate(words):\n",
    "    ax.annotate(word, (x[i],y[i]))\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('True Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], color = colors_p, alpha = 0.5)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('Predicted Clusters\\n k = 4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = newsgroupsTFVects.toarray()\n",
    "plotSilhouette(2, X)\n",
    "\n",
    "plotSilhouette(3, X)\n",
    "\n",
    "plotSilhouette(4, X)\n",
    "\n",
    "\n",
    "plotSilhouette(5, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ngTFVectorizer.get_feature_names()\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(numClusters):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroupsTFVects[:100].todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroupsCoocMat = newsgroupsTFVects * newsgroupsTFVects.T\n",
    "#set the diagonal to 0 since we don't care how similar texts are to themselves\n",
    "newsgroupsCoocMat.setdiag(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "selectIndices = []\n",
    "indexToCat = []\n",
    "for c in set(df['pair']):\n",
    "    selectIndices += list(df[df['pair'] == c].index)[:1000]\n",
    "    indexToCat += [c] * 50\n",
    "    #.groupby('category').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subCoocMat = newsgroupsCoocMat[selectIndices,:][:,selectIndices]\n",
    "linkage_matrix = scipy.cluster.hierarchy.ward(subCoocMat.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendDat = scipy.cluster.hierarchy.dendrogram(linkage_matrix, get_leaves=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label and see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply our functions\n",
    "senReleasesTraining = plot_data\n",
    "\n",
    "senTFVectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_df=100, min_df=2, stop_words='english', norm='l2')\n",
    "senTFVects = senTFVectorizer.fit_transform(senReleasesTraining['text'])\n",
    "#senTFVectorizer.vocabulary_.get('senat', 'Missing \"Senate\"')\n",
    "\n",
    "senReleasesTraining['tokenized_text'] = senReleasesTraining['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "senReleasesTraining['normalized_tokens'] = senReleasesTraining['tokenized_text'].apply(lambda x: lucem_illud.normalizeTokens(x, \n",
    "                                                                                                                            stopwordLst = lucem_illud.stop_words_basic, \n",
    "                                                                                                                            stemmer = lucem_illud.stemmer_basic))\n",
    "def dropMissing(wordLst, vocab):\n",
    "    return [w for w in wordLst if w in vocab]\n",
    "\n",
    "senReleasesTraining['reduced_tokens'] = senReleasesTraining['normalized_tokens'].apply(lambda x: dropMissing(x, senTFVectorizer.vocabulary_.keys()))\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(senReleasesTraining['reduced_tokens'])\n",
    "corpus = [dictionary.doc2bow(text) for text in senReleasesTraining['reduced_tokens']]\n",
    "\n",
    "gensim.corpora.MmCorpus.serialize('senate.mm', corpus)\n",
    "senmm = gensim.corpora.MmCorpus('senate.mm')\n",
    "\n",
    "senlda = gensim.models.ldamodel.LdaModel(corpus=senmm, id2word=dictionary, num_topics=10, alpha='auto', eta='auto')\n",
    "#senReleasesTraining.reset_index(inplace = True)\n",
    "sen1Bow = dictionary.doc2bow(senReleasesTraining['reduced_tokens'][0])\n",
    "sen1lda = senlda[sen1Bow]\n",
    "print(\"The topics of the text: {}\".format(senReleasesTraining['headline'][0]))\n",
    "print(\"are: {}\".format(sen1lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#senReleasesTraining.reset_index(inplace = True)\n",
    "sen1Bow = dictionary.doc2bow(senReleasesTraining['reduced_tokens'][0])\n",
    "sen1lda = senlda[sen1Bow]\n",
    "print(\"The topics of the text: {}\".format(senReleasesTraining['headline'][0]))\n",
    "print(\"are: {}\".format(sen1lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaDF = pandas.DataFrame({\n",
    "        'name' : senReleasesTraining['headline'],\n",
    "        'topics' : [senlda[dictionary.doc2bow(l)] for l in senReleasesTraining['reduced_tokens']]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dict to temporally hold the probabilities\n",
    "topicsProbDict = {i : [0] * len(ldaDF) for i in range(senlda.num_topics)}\n",
    "\n",
    "#Load them into the dict\n",
    "for index, topicTuples in enumerate(ldaDF['topics']):\n",
    "    for topicNum, prob in topicTuples:\n",
    "        topicsProbDict[topicNum][index] = prob\n",
    "\n",
    "#Update the DataFrame\n",
    "for topicNum in range(senlda.num_topics):\n",
    "    ldaDF['topic_{}'.format(topicNum)] = topicsProbDict[topicNum]\n",
    "\n",
    "ldaDF[1::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaDFV = ldaDF[:10][['topic_%d' %x for x in range(3)]]\n",
    "ldaDFVisN = ldaDF[:10][['name']]\n",
    "ldaDFVis = ldaDFV.as_matrix(columns=None)\n",
    "ldaDFVisNames = ldaDFVisN.as_matrix(columns=None)\n",
    "ldaDFV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senlda.show_topic(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senlda.show_topic(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senlda.show_topic(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "#For NLP\n",
    "import nltk\n",
    "\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "\n",
    "#Displays the graphs\n",
    "import graphviz #You also need to install the command line graphviz\n",
    "\n",
    "#These are from the standard library\n",
    "import os.path\n",
    "import zipfile\n",
    "import subprocess\n",
    "import io\n",
    "import tempfile\n",
    "#import lucem_illud.stanford as stanford\n",
    "#lucem_illud.setupStanfordNLP()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
